{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5e8a02-6d56-4ee6-8589-5ec1339bacff",
   "metadata": {},
   "source": [
    "# Video Cut Tool\n",
    "\n",
    "Building on the model's temporal grounding and reasoning capabilities, we have further expanded its tool-use capabilities in video reasoning and designed the **VideoCut** tool. During inference, the model can **replay certain clips** to clearly watch details in the video, thereby improving reasoning accuracy.\n",
    "\n",
    "During the inference process, the model outputs the **start and end times of clips** to be rewatched (supporting multiple clips) and the desired **slowdown frame rate** (currently supporting 1-5 FPS). VideoCut resamples the video clips based on the start/end times and frame rate, enabling the model to clearly perceive details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Setup the environment\n",
    "Import necessary libraries and set up API access and client configuration for model inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881633f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Please set the API key here\n",
    "import os\n",
    "\n",
    "os.environ['ARK_API_KEY']  = 'your_ark_api_key'\n",
    "os.environ['ARK_MODEL_ENDPOINT'] = \"doubao-seed-1-8-251215\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "from video_processing import process_video, sample_frames_from_video_bytes\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://ark.cn-beijing.volces.com/api/v3\",\n",
    "    api_key=os.environ.get(\"ARK_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b2373",
   "metadata": {},
   "source": [
    "### 1. VideoCut Function Definition\n",
    "Define the tool that clips and resamples segments for clearer perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4932d0a9-b3d1-4b30-b795-58a8ce953657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videocut(\n",
    "    video: str | bytes,\n",
    "    timestamps: str,\n",
    "    fps: int = 1,\n",
    "    video_max_sequence_length: int = 32768,\n",
    "    token_sets: tuple = (32, 64, 96, 128, 160, 192, 224, 256),\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    \"name\": \"VIDEOCUT\",\n",
    "    \"description\": \"Clips a video segment based on precise timestamps and FPS parameters for clearer playback.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"timestamps\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The timestamps (two float numbers indicate start and end seconds) determines where to cut the video. It is a string of the form 's1 - s2' seconds. The start_time and end_time are floating-point numbers representing the absolute start and end times of the segment in seconds.\"\n",
    "            },\n",
    "            \"fps\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"An integer number from 1 to 5, representing the sampling FPS for video clip. If not provided, the video will be sampled by 1 FPS by default.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"timestamps\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "    \"\"\"\n",
    "    if isinstance(video, str):\n",
    "        with open(video, 'rb') as f:\n",
    "            video_bytes = f.read()\n",
    "    elif isinstance(video, bytes):\n",
    "        video_bytes = video\n",
    "    else:\n",
    "        raise ValueError('Invalid video type. Must be str or bytes.')\n",
    "\n",
    "    # check timestamps, multiple timestamps or single timestamps\n",
    "    timestamps = timestamps.split(',')\n",
    "    timestamps = [m.strip() for m in timestamps]\n",
    "    output_frame_bytes = []\n",
    "    output_timestamps = []\n",
    "\n",
    "    # cut video clips from timestamps\n",
    "    sampling_segments = []\n",
    "    num_frames_estimated = 0\n",
    "    for orig_timestamp in timestamps:\n",
    "        pattern = r'\\d+(?:\\.\\d+)?'\n",
    "        timestamp = re.findall(pattern, orig_timestamp)\n",
    "        timestamp = [float(m) for m in timestamp]\n",
    "        sample_fps = int(fps)\n",
    "        if len(timestamp) != 2:\n",
    "            raise ValueError('Invalid segment format. Must be s1 - s2 seconds. {}'.format(orig_timestamp))\n",
    "        start_sec, end_sec = timestamp\n",
    "        sampling_segments.append((start_sec, end_sec))\n",
    "        num_frames_estimated += int((end_sec - start_sec) * sample_fps)\n",
    "\n",
    "    max_frame_token = token_sets[0]\n",
    "    for token in token_sets[1:]:\n",
    "        if token * num_frames_estimated <= video_max_sequence_length:\n",
    "            max_frame_token = token\n",
    "\n",
    "    for start_sec, end_sec in sampling_segments:\n",
    "        sampled_frames, sampled_timestamps = sample_frames_from_video_bytes(\n",
    "            video_bytes,\n",
    "            start_sec,\n",
    "            end_sec,\n",
    "            sample_fps,\n",
    "            tokens_per_image=max_frame_token,\n",
    "        )\n",
    "        output_frame_bytes.extend(sampled_frames)\n",
    "        output_timestamps.extend(sampled_timestamps)\n",
    "\n",
    "    return output_frame_bytes, output_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a447b",
   "metadata": {},
   "source": [
    "Define the tool schema for VideoCut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_schemas = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"VIDEOCUT\",\n",
    "        \"description\": \"Clips a video segment based on precise timestamps and FPS parameters for clearer playback.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"timestamps\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The timestamps (two float numbers indicate start and end seconds) determines where to cut the video. It is a string of the form 's1 - s2' seconds. The start_time and end_time are floating-point numbers representing the absolute start and end times of the segment in seconds.\"\n",
    "                },\n",
    "                \"fps\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"An integer number from 1 to 5, representing the sampling FPS for video clip. If not provided, the video will be sampled by 1 FPS by default.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"timestamps\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14a275",
   "metadata": {},
   "source": [
    "### 2. Preprocess Video\n",
    "Sample initial frames from the full video to bootstrap understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db41591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path,\n",
    "                     sampling_fps=1,\n",
    "                     max_frames=1280,\n",
    "                     max_video_length=81920):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise ValueError('Video file does not exist.')\n",
    "    with open(video_path, 'rb') as f:\n",
    "        video_bytes = f.read()\n",
    "\n",
    "    video_base64_list, timestamps, _ = process_video(video_bytes, sampling_fps,\n",
    "                                                     max_frames,\n",
    "                                                     max_video_length)\n",
    "    return video_base64_list, timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a130a",
   "metadata": {},
   "source": [
    "### 3. Construct Video Message\n",
    "Compose a user message with sampled frames and timestamps for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5b3aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_video_message(prompt, video_frames, video_timestamps):\n",
    "    \"\"\"\n",
    "    Construct a message for video understanding.\n",
    "    \"\"\"\n",
    "    video_contents = []\n",
    "    for image_bytes, timestamp in zip(video_frames, video_timestamps):\n",
    "        video_contents.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'[{round(timestamp, 1)} second]'\n",
    "        })\n",
    "        video_contents.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{image_bytes}\",\n",
    "        })\n",
    "    contents = video_contents + [{\"type\": \"text\", \"text\": prompt}]\n",
    "    message = [{\"role\": \"user\", \"content\": contents}]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bdf0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_complete(client, messages, with_tool=False):\n",
    "    if with_tool and messages[0]['role'] == 'user':\n",
    "        # add hint\n",
    "        messages.insert(\n",
    "            0, {\n",
    "                \"role\":\n",
    "                \"system\",\n",
    "                \"content\":\n",
    "                \"You can use available tools, such as VIDEOCUT, to help you perceive the video more accurately.\"\n",
    "            })\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.environ.get('ARK_MODEL_ENDPOINT'),\n",
    "        messages=messages,\n",
    "        reasoning_effort=\"medium\",\n",
    "        tools=tool_schemas if with_tool else None,\n",
    "        stream=False,\n",
    "        max_completion_tokens=32768)\n",
    "    print(response)\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"{}\".format(response.choices[0].message.content)\n",
    "    })\n",
    "    return response.choices[0].message, messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb9b38",
   "metadata": {},
   "source": [
    "### 4. Video Understanding with VideoCut\n",
    "Use a multi-turn loop to improve video analysis. The model can then request to `VIDEOCUT` specific segments at a higher FPS to 'rewatch' moments of interest. This is ideal for tasks requiring detailed observation that might be missed in the initial low-fps view. By feeding these high-detail clips back to the model, you enable it to refine its understanding and provide more accurate answers for complex video questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_detect_execute(response, video_path):\n",
    "    if response.tool_calls is not None and len(response.tool_calls) > 0:\n",
    "        # one call per turn\n",
    "        function = response.tool_calls[0].function\n",
    "        function_name = function.name\n",
    "        function_args = json.loads(function.arguments)\n",
    "        if function_name == 'VIDEOCUT':\n",
    "            timestamps = function_args['timestamps']\n",
    "            fps = int(function_args.get('fps', 1))\n",
    "            print(\"[Call VIDEOCUT] timestamps: {}, fps: {}\".format(timestamps, fps))\n",
    "            with open(video_path, 'rb') as f:\n",
    "                video_bytes = f.read()\n",
    "            video_frames, video_timestamps = videocut(video_bytes, timestamps, fps)\n",
    "            video_contents = []\n",
    "            for image_bytes, timestamp in zip(video_frames, video_timestamps):\n",
    "                video_contents.append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f'[{round(timestamp, 1)} second]'\n",
    "                })\n",
    "                video_contents.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{image_bytes}\",\n",
    "                })\n",
    "            tool_messages = [{\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": response.tool_calls[0].id,\n",
    "                \"content\": video_contents\n",
    "            }]\n",
    "            return tool_messages\n",
    "        else:\n",
    "            raise ValueError(f'Invalid function name: {function_name}')\n",
    "    else:\n",
    "        print(\"No tool call detected.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[video] 1643.8 second video, sampling 768 frames, 64 tokens per frame\n",
      "ChatCompletion(id='021765528188121ce5ecf588ca94db1d51bc7611e0e34a2d80012', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_fzgpz0np8j2b2205cvenykpj', function=Function(arguments='{\"timestamps\":\"1075.9 - 1095.2 seconds\",\"fps\":2}', name='VIDEOCUT'), type='function')], reasoning_content=\"Got it, let's look at the part around 1078.0 to 1093.0 seconds. Wait, actually 1080.1 to 1090.9 seconds, Chaplin hides the coat under the blanket on the bed? No, wait let's use VIDEOCUT on timestamps 1075.9 - 1095.2 seconds with fps 2 to check.\"))], created=1765528204, model='doubao-seed-1-8-preview-251115', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=144, prompt_tokens=56808, total_tokens=56952, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=96, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "<think>Got it, let's look at the part around 1078.0 to 1093.0 seconds. Wait, actually 1080.1 to 1090.9 seconds, Chaplin hides the coat under the blanket on the bed? No, wait let's use VIDEOCUT on timestamps 1075.9 - 1095.2 seconds with fps 2 to check.</think>\n",
      "[Call VIDEOCUT] timestamps: 1075.9 - 1095.2 seconds, fps: 2\n",
      "ChatCompletion(id='021765528204966ce5ecf588ca94db1d51bc7611e0e34a27e6d9f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To prevent theft, Chaplin hid the coat **under his body, beneath the blanket on the bed** (covering himself with the blanket while concealing the coat underneath).', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=\"Got it, let's look at the video segment. Chaplin hid the coat under his body (under the blanket on the bed) to prevent theft.\"))], created=1765528218, model='doubao-seed-1-8-preview-251115', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=66, prompt_tokens=67074, total_tokens=67140, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=32, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "<think>Got it, let's look at the video segment. Chaplin hid the coat under his body (under the blanket on the bed) to prevent theft.</think>\n",
      "No tool call detected.\n",
      "Seed1.8:\n",
      "<think>Got it, let's look at the video segment. Chaplin hid the coat under his body (under the blanket on the bed) to prevent theft.</think>\n",
      "To prevent theft, Chaplin hid the coat **under his body, beneath the blanket on the bed** (covering himself with the blanket while concealing the coat underneath).\n"
     ]
    }
   ],
   "source": [
    "video_path = \"samples/Chaplin_512kb.mp4\"\n",
    "# you may need to download the videofrom https://ia800606.us.archive.org/15/items/CharlieChaplin/Chaplin_512kb.mp4\n",
    "text_prompts = \"To prevent theft, where did Chaplin hide the coat?\"\n",
    "# sampling video frames\n",
    "sampling_fps = 1\n",
    "sampled_frames, timestamps = preprocess_video(video_path, max_video_length=49152)\n",
    "messages = construct_video_message(prompt=text_prompts, video_timestamps=timestamps, video_frames=sampled_frames)\n",
    "result, history_messages = api_complete(client, messages, with_tool=True)\n",
    "response = result.content\n",
    "reasoning_content = result.reasoning_content\n",
    "print(\"<think>{}</think>\".format(reasoning_content))\n",
    "tool_message = tool_detect_execute(result, video_path)\n",
    "if tool_message is not None:\n",
    "    history_messages.extend(tool_message)\n",
    "    use_tool = True\n",
    "else:\n",
    "    use_tool = False\n",
    "\n",
    "while use_tool:\n",
    "    result, history_messages = api_complete(client, history_messages, with_tool=True)\n",
    "    response = result.content\n",
    "    reasoning_content = result.reasoning_content\n",
    "    print(\"<think>{}</think>\".format(reasoning_content))\n",
    "    tool_message = tool_detect_execute(result, video_path)\n",
    "    if tool_message is not None:\n",
    "        history_messages.extend(tool_message)\n",
    "        use_tool = True\n",
    "    else:\n",
    "        use_tool = False\n",
    "    \n",
    "print(\"Seed1.8:\\n<think>{}</think>\\n{}\".format(reasoning_content, response))\n"
   ]
  }
 ],
 "metadata": {
  "fileId": "7ba2c980-8f34-4e7d-b034-cde73e9e537a",
  "filePath": "/mnt/bn/ic-vlm/personal/chengtianheng/Seed-1.8/Video/video_tool.ipynb",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
